import numpy as np
import pandas as pd 

from emot.emo_unicode import UNICODE_EMO, EMOTICONS
import emoji
import unicodedata
import regex as re
import copy
import string
from vncorenlp import VnCoreNLP
from gensim.parsing.preprocessing import strip_non_alphanum, split_alphanum, strip_short, strip_numeric
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# !pip3 install fairseq
# !pip3 install fastbpe
# !pip3 install vncorenlp
# !pip3 install transformers
# !pip install emot
# !pip install emoji

# >>>Chu·∫©n h√≥a c√°ch ƒë·∫∑t d·∫•u ki·ªÉu c≈© -> m·ªõi. V√≠ d·ª• "q·ªßa"->"qu·∫£"
# >>>V√≠ d·ª•: 
# >>>test_string = "to·∫£n t·ªèan t·ªèa to·∫£ qu·∫£ q·ªßa qu·∫ø q√∫√™"
# >>>print(test_string)
# >>>standardize_vi_sentence_accents(test_string)

# => to·∫£n t·ªèan t·ªèa to·∫£ qu·∫£ q·ªßa qu·∫ø q√∫√™
#     'to·∫£n to·∫£n t·ªèa t·ªèa qu·∫£ qu·∫£ qu·∫ø qu·∫ø'

bang_nguyen_am = [['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a'],
                  ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],
                  ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],
                  ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e'],
                  ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],
                  ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i'],
                  ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o'],
                  ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],
                  ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],
                  ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u'],
                  ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],
                  ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y']]
bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']

nguyen_am_to_ids = {}

for i in range(len(bang_nguyen_am)):
    for j in range(len(bang_nguyen_am[i]) - 1):
        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)

"""
    Start section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ c√°ch g√µ d·∫•u ki·ªÉu c≈©: d√πng √≤a √∫y thay o√† u√Ω
    Xem t·∫°i ƒë√¢y: https://vi.wikipedia.org/wiki/Quy_t·∫Øc_ƒë·∫∑t_d·∫•u_thanh_trong_ch·ªØ_qu·ªëc_ng·ªØ
            C≈©	                M·ªõi
    √≤a, √≥a, ·ªèa, √µa, ·ªça	|o√†, o√°, o·∫£, o√£, o·∫°
    √≤e, √≥e, ·ªèe, √µe, ·ªçe	|o√®, o√©, o·∫ª, o·∫Ω, o·∫π
    √πy, √∫y, ·ªßy, ≈©y, ·ª•y	|u·ª≥, u√Ω, u·ª∑, u·ªπ, u·ªµ
"""

def standardize_vi_word_accents(word):
    if not is_valid_vietnam_word(word):
        return word

    chars = list(word)
    dau_cau = 0
    nguyen_am_index = []
    qu_or_gi = False
    for index, char in enumerate(chars):
        x, y = nguyen_am_to_ids.get(char, (-1, -1))
        if x == -1:
            continue
        elif x == 9:  # check qu
            if index != 0 and chars[index - 1] == 'q':
                chars[index] = 'u'
                qu_or_gi = True
        elif x == 5:  # check gi
            if index != 0 and chars[index - 1] == 'g':
                chars[index] = 'i'
                qu_or_gi = True
        if y != 0:
            dau_cau = y
            chars[index] = bang_nguyen_am[x][0]
        if not qu_or_gi or index != 1:
            nguyen_am_index.append(index)
    if len(nguyen_am_index) < 2:
        if qu_or_gi:
            if len(chars) == 2:
                x, y = nguyen_am_to_ids.get(chars[1])
                chars[1] = bang_nguyen_am[x][dau_cau]
            else:
                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))
                if x != -1:
                    chars[2] = bang_nguyen_am[x][dau_cau]
                else:
                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]
            return ''.join(chars)
        return word

    for index in nguyen_am_index:
        x, y = nguyen_am_to_ids[chars[index]]
        if x == 4 or x == 8:  # √™, ∆°
            chars[index] = bang_nguyen_am[x][dau_cau]
            return ''.join(chars)

    if len(nguyen_am_index) == 2:
        if nguyen_am_index[-1] == len(chars) - 1:
            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]
            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]
        else:
            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]
            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]
    else:
        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]
        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]
    return ''.join(chars)

def is_valid_vietnam_word(word):
    chars = list(word)
    nguyen_am_index = -1
    for index, char in enumerate(chars):
        x, y = nguyen_am_to_ids.get(char, (-1, -1))
        if x != -1:
            if nguyen_am_index == -1:
                nguyen_am_index = index
            else:
                if index - nguyen_am_index != 1:
                    return False
                nguyen_am_index = index
    return True


def standardize_vi_sentence_accents(sentence):
    sentence = sentence.lower()
    words = sentence.split()
    for index, word in enumerate(words):
        cw = re.sub(r'(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)', r'\1/\2/\3', word).split('/')
        if len(cw) == 3:
            cw[1] = standardize_vi_word_accents(cw[1])
        words[index] = ''.join(cw)
    return ' '.join(words)

# Thay email, c√°c ƒë∆∞·ªùng d·∫´n link th√†nh MAIL, URL;  
# Chuy·ªÉn emoij, emotion sang bi·ªÉu di·ªÖn t·ª´ c√≥ nghƒ©a;
# Chu·∫©n h√≥a t·ª´ b·ªã l·∫∑p l·∫°i ki·ªÉu "c∆°mmmm th√¥i" hay "th√¥ii:

# VD:
# >>>test_mail = "khai@gmail.com vcsdl@gd.sr.hk bdf@ydf.df.com"
# >>>test_link = "https:\\med.com.vn  www.hoag.com.vn uet.vnu.vn"
# >>>test_emoji_emotion = "üòÇüòÇ :v :) ^_^ ^-^ @_@"
# >>>print(replace_email(test_mail))
# >>>print(replace_link(test_link))
# >>>print(convert_emojis(test_emoji_emotion))
# >>>print(convert_emotions(test_emoji_emotion))

# EMAIL EMAIL EMAILemojis.get_emoji_regexp()
# https:\URL
# :face_with_tears_of_joy::face_with_tears_of_joy: :v :) ^_^ ^-^ @_@
# üòÇüòÇ :v Happy_face_or_smiley Joyful ^-^ @_@

def replace_email(text_str):
  re_email = r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b'
  match = re.findall(re_email ,text_str,re.IGNORECASE)
  if not match:
    return text_str
  return re.sub("|".join(match),"EMAIL",text_str)

def replace_link(text_str):
  re_link = r"(https?:\/\/)?([\da-z\.-]+)\.([a-z\.]{2,6})([\/\w \.-]*)"
  return re.sub(re_link, 'URL', text_str)

def replace_emoji(text_str):

  return emoji.get_emoji_regexp().sub(r' EMOJI ', text_str)
  
def convert_emojis(text):
    for emot in UNICODE_EMO:
        text = text.replace(emot, "_".join(UNICODE_EMO[emot].split()))
    return text

def convert_emotions(text):
    for emot in EMOTICONS:
        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)
    return text

#******************************************************************************************************************
# lo·∫°i b·ªè stop words b·∫±ng vietnamese-stopwords
# Note: file ƒë·∫ßu v√†o d·∫°ng lowcase
def get_stop_words(stopwords_file = "./vietnamese-stopwords/vietnamese-stopwords.txt"):
    with open(stopwords_file, "r", encoding='utf8') as file_sw:
        stop_word = file_sw.read().split(sep='\n')
        # ƒë∆∞a v·ªÅ word d·∫°ng "xin ch√†o" th√†nh "xin_ch√†o" t∆∞∆°ng ·ª©ng v·ªõi b·ªô tokenize c·ªßa vncorenlp
        stop_word = [word.replace(" ", "_") for word in stop_word]
    return stop_word

def get_teencode_dict(teencode_file = "./dict_topic_word/teencode.txt"):
    teencode = pd.read_csv(teencode_file, sep="\t",
                            header=None, names=["teencode", "standard_word"])
    teencode_dict = teencode.set_index("teencode").to_dict()['standard_word']
    return teencode_dict

# Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
# text: ·ªü ƒë√¢y l√† m·ªôt document c√≥ th·ªÉ g·ªìm nhi·ªÅu c√¢u
# ta s·∫Ω th·ª±c hi·ªán ti·ªÅn x·ª≠ l√Ω chu·∫©n h√≥a, clean,... ƒë·∫øn cu·ªëi c√πng s·∫Ω tokenize c·∫£
# vƒÉn b·∫£n ban ƒë·∫ßu (ch·ª© kh√¥ng ph·∫£i l√†m theo t·ª´ng c√¢u), r·ªìi l·∫°i gh√©p l·∫°i l√†m 1 "c√¢u"
# ƒë·∫°i di·ªán cho 1 vƒÉn b·∫£n c√≥ c√°c t·ª´ gh√©p n·ªëi v·ªõi nhau b·ªüi "_" 2 t·ª´ (t·ª´ gh√©p) ph√¢n 
# nhau b·ªüi d·∫•u space.
# (Li·ªáu c√≥ ph·∫£i vncorenlp n√≥ hu·∫•n luy√™n tr√™n t·ª´ng c√¢u m√† gi·ªù m√¨nh tokenize c·∫£ vƒÉn
# b·∫£n thay t·ª´ng c√¢u th√¨ c√≥ v·∫•n ƒë·ªÅ g√¨ kh√¥ng??)

# ƒë·ªÉ sau n√†y embedding. ·ªû ƒë√¢y ta ƒëang ch∆°i embedding c·∫£ vƒÉn b·∫£n, b√†i vƒÉn ch·ª© kh√¥ng
# embedding theo t·ª´ng c√¢u.

# To perform word segmentation only
annotator = VnCoreNLP("VnCoreNLP/VnCoreNLP-1.1.1.jar", annotators="wseg", max_heap_size='-Xmx500m')

def remove_dup_char(text):
  return re.sub(r'(.)\1+', r'\1', text)

def text_preprocessing(text, annotator=annotator, stopwords=[]):
    # kwargs: annotator, stopwords
  origin_text = copy.deepcopy(text)  
  try:
    text = unicodedata.normalize("NFC", text)
    text = text.lower()

    # Chu·∫©n h√≥a c√°c t·ª´ c√≥ k√≠ t·ª± b·ªã l·∫∑p l·∫°i
    text = remove_dup_char(text)

    text = replace_email(text)
    text = replace_emoji(text)

    # Thay th·∫ø t·∫•t c·∫£ c√°c ƒë∆∞·ªùng d·∫´n trong vƒÉn b·∫£n.
    text = replace_link(text)
    
    #remove punctuation
    table = str.maketrans('', '',string.punctuation)
    text = text.translate(table)

    # T√°ch mix t·ª´ v√† s·ªë
    text = split_alphanum(text)
    # Lo·∫°i b·ªè to√†n b·ªô c√°c k√Ω t·ª± ƒë·ª©ng 1 m√¨nh
    text = strip_short(text, minsize=2)
    # Lo·∫°i b·ªè h·∫øt c√°c s·ªë trong vƒÉn b·∫£n
    text = strip_numeric(text)

    # Lo·∫°i b·ªè c√°c k√≠ t·ª± l·∫° nh∆∞ ‚â•
    text = strip_non_alphanum(text)
    # Chu·∫©n h√≥a c√°ch ƒë·∫∑t d·∫•u cho c√°c t·ª´ trong c√¢u: v√≠ d·ª• "nh√¢n q·ªßa" -> "nh√¢n qu·∫£" 
    text = standardize_vi_sentence_accents(text)
    

    text = text.strip()
    if not text:
      return "EMPTY_STRING"
    #tokenize
    # lu√¥n ch·ªâ c√≥ 1 ph·∫ßn t·ª´ n√™n l·∫•y [0]
    # l√Ω do ch·ªâ c√≥ 1 ph·∫ßn t·ª≠ l√† v√¨ ta ƒë√£ x√≥a h·∫øt d·∫•u c√¢u c·∫£ vƒÉn b·∫£n coi l√† 1 c√¢u
    # kh√¥ng c√≤n d·∫•u "." n√™n b·ªô tokenize xem nh∆∞ l√† 1 c√¢u n√™n ch·ªâ c√≥ 1 ph·∫ßn t·ª≠.
    list_token = annotator.tokenize(text)[0]

    # Chu·∫©n h√≥a teencode
    # list_token = [teencode_dict.get(token, token) for token in list_token]

    # remove stopword
    list_token = [token for token in list_token if token not in stopwords]

    norm_text = " ".join(list_token)

    return norm_text
  except:
    print("EXCEPTION with", origin_text, '\n')
    return "EXCEPTION_STRING"

############################################################################
############################  BERRT  ########################################
from fairseq.data.encoders.fastbpe import fastBPE
from fairseq.data import Dictionary
import argparse
from tqdm import tqdm

def build_vocab(path_bpe="PhoBERT_base_transformers/bpe.codes" ,path_dict="PhoBERT_base_transformers/dict.txt"):
  parser = argparse.ArgumentParser()
  parser.add_argument('--bpe-codes', 
    default= path_bpe,
    required=False,
    type=str,
    help='path to fastBPE BPE'
  )
  args, unknown = parser.parse_known_args()
  bpe = fastBPE(args)
  # Load the dictionary
  vocab = Dictionary()
  vocab.add_from_file(path_dict)
  return vocab, bpe

from tensorflow.keras.preprocessing.sequence import pad_sequences
def convert_line(sents, MAX_LEN=256):
  vocab, bpe = build_vocab()
  ids = []
  for sent in sents:
    subwords = '<s> ' + bpe.encode(sent) + ' </s>'
    encoded_sent = vocab.encode_line(subwords, append_eos=True, add_if_not_exist=False).long().tolist()
    ids.append(encoded_sent)
  i_d = pad_sequences(ids, maxlen=MAX_LEN, dtype="long", value=0, truncating="post", padding="post")
  return i_d

# Create Masks
def create_masks(ids):
  masks = []
  for sent in ids:
    mask = [int(token_id > 0) for token_id in sent]
    masks.append(mask)
  return masks

# Load DataLoader
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
import torch

def Load_dataloader(ids, labels, masks, batch_size=32):
  inputs = torch.LongTensor(ids)
  labels = torch.tensor(labels)
  masks = torch.tensor(masks)

  data = TensorDataset(inputs, masks, labels)
  sampler = SequentialSampler(data)
  dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)

  return dataloader

# Evaluate
import numpy as np
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix

def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    
    F1_score = f1_score(pred_flat, labels_flat, average='macro')
    cm = confusion_matrix(pred_flat, labels_flat)
    
    return accuracy_score(pred_flat, labels_flat), F1_score, cm

# preprocessing for bert
def bert_preprocessing(train_sents, train_labels, test_text, test_labels, w2v=1):
  # split train and valid set
  train_sents, val_sents, train_labels, val_labels = train_test_split(train_sents, train_labels, test_size=0.1, random_state=42)
  vectorizer = TfidfVectorizer(max_features=128)
  train_tfidf = vectorizer.fit_transform(train_sents).toarray()
  val_tfidf = vectorizer.transform(val_sents).toarray()
  test_tfidf = vectorizer.transform(test_text).toarray()

  # Convert sentence to vector
  if w2v == 'bpe_tfidf': # bpe + tfidf
    # bpe
    train_ids = convert_line(train_sents, MAX_LEN=128)
    val_ids = convert_line(val_sents, MAX_LEN=128)
    test_ids = convert_line(test_text, MAX_LEN=128)

    # concate tfidf with bpe
    train_ids = np.concatenate((train_ids, train_tfidf*100), axis=1)
    val_ids = np.concatenate((val_ids, val_tfidf*100), axis=1)
    test_ids = np.concatenate((test_ids, test_tfidf*100), axis=1)

  elif w2v == 'bpe':   # only bpe
    train_ids = convert_line(train_sents, MAX_LEN=256)
    val_ids = convert_line(val_sents, MAX_LEN=256)
    test_ids = convert_line(test_text, MAX_LEN=256)

  elif w2v == 'tfidf':
    train_ids = train_tfidf*100
    val_ids = val_tfidf*100
    test_ids = test_tfidf*100
  else:
    print('Enter again')
    return _

  # creat masks
  train_masks = create_masks(train_ids)
  val_masks = create_masks(val_ids)
  test_masks =create_masks(test_ids)

  # Load DataLoader
  train_dataloader = Load_dataloader(train_ids, train_labels, train_masks)
  val_dataloader = Load_dataloader(val_ids, val_labels, val_masks)
  test_dataloader = Load_dataloader(test_ids, test_labels, test_masks)

  return train_dataloader, val_dataloader, test_dataloader, train_sents, val_sents, train_labels, val_labels, vectorizer